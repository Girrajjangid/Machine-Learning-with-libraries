{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import joblib\n",
    "import warnings\n",
    "from APIs import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TZ'] = 'Asia/Calcutta'\n",
    "pd.set_option('display.max_columns', 100)\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (20, 6)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import plot_importance\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.metrics import r2_score, accuracy_score, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-24 00:00:00 2019-12-26 00:00:00 2019-12-10 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>apparent_temperature</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_bearing</th>\n",
       "      <th>precip_probability</th>\n",
       "      <th>precip_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6175</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.91</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>310.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       datetime  cloud_cover  apparent_temperature  temperature  humidity  \\\n",
       "6175 2019-12-25          0.3                   8.5          8.5      0.91   \n",
       "\n",
       "      dew_point  wind_speed  wind_bearing  precip_probability  \\\n",
       "6175        6.8         2.0         310.5                 0.0   \n",
       "\n",
       "      precip_intensity  \n",
       "6175               0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 25 # This is todays date \n",
    "raw_df = pickle.load(open('tpddl_data','rb'))\n",
    "train_date  =  pd.to_datetime(\"2019-12-\"+str(i-1))\n",
    "prediction_date = pd.to_datetime(\"2019-12-\"+str(i+1))\n",
    "validation_date = pd.to_datetime(\"2019-12-\"+str(i-15))\n",
    "print(train_date, prediction_date, validation_date)\n",
    "\n",
    "wdf_forecast = pickle.load(open(\"tpddl_weather_forecast_data\",\"rb\"))\n",
    "wdf_forecast_data = wdf_forecast[(wdf_forecast.datetime.dt.date>=prediction_date-timedelta(1))&(wdf_forecast.datetime.dt.date<=prediction_date)]\n",
    "wdf_forecast_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['datetime_local'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-4a2719341377>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tpddl_weather_historical_all_data_2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m wdf = wdf[[\"datetime_local\", 'apparent_temperature', 'temperature', 'humidity', 'dew_point', \n\u001b[1;32m----> 3\u001b[1;33m                  'wind_speed', 'cloud_cover', 'wind_bearing', 'precip_probability', 'precip_intensity']].copy()\n\u001b[0m\u001b[0;32m      4\u001b[0m weather_feature = ['apparent_temperature', 'temperature', 'humidity','dew_point', 'wind_speed',\n\u001b[0;32m      5\u001b[0m                    'cloud_cover','wind_bearing','precip_probability', 'precip_intensity']\n",
      "\u001b[1;32mC:\\Users\\GirrajJangid\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2984\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2986\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GirrajJangid\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                 \u001b[1;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"raise_missing\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GirrajJangid\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1092\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GirrajJangid\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"loc\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not in index\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['datetime_local'] not in index\""
     ]
    }
   ],
   "source": [
    "wdf = pickle.load(open(\"tpddl_weather_historical_all_data\",\"rb\"))\n",
    "wdf = wdf[[\"datetime_local\", 'apparent_temperature', 'temperature', 'humidity', 'dew_point', \n",
    "                 'wind_speed', 'cloud_cover', 'wind_bearing', 'precip_probability', 'precip_intensity']].copy()\n",
    "weather_feature = ['apparent_temperature', 'temperature', 'humidity','dew_point', 'wind_speed',\n",
    "                   'cloud_cover','wind_bearing','precip_probability', 'precip_intensity']\n",
    "\n",
    "wdf.rename(columns= {'datetime_local': \"datetime\"},inplace=True)\n",
    "wdf['datetime'] = pd.to_datetime(wdf['datetime'])\n",
    "wdf.iloc[:,1:] = wdf.iloc[:,1:].astype(\"float64\")\n",
    "wdf.dropna(inplace=True)\n",
    "wdf.drop_duplicates(\"datetime\", inplace=True)\n",
    "wdf = wdf[(wdf.datetime.dt.year <= 2019) & (wdf.datetime.dt.year >= 2015)]\n",
    "\n",
    "\n",
    "# Outlier removing\n",
    "wdf.loc[wdf['temperature'] < 0,   'temperature']          = np.nan\n",
    "wdf.loc[wdf.temperature.isnull(), 'dew_point']            = np.nan\n",
    "wdf.loc[wdf.temperature.isnull(), 'apparent_temperature'] = np.nan\n",
    "wdf.loc[wdf.temperature.isnull(), 'humidity']    = np.nan\n",
    "wdf.loc[wdf.temperature.isnull(), 'wind_speed']  = np.nan\n",
    "wdf.loc[wdf.temperature.isnull(), 'cloud_cover'] = np.nan\n",
    "\n",
    "wdf.loc[(wdf['wind_bearing'] > 359.0) & (wdf['wind_bearing'] < 0),  'wind_bearing'] = np.nan\n",
    "wdf.loc[wdf.wind_bearing.isnull(), 'dew_point']   = np.nan\n",
    "wdf.loc[wdf.wind_bearing.isnull(), 'apparent_temperature'] = np.nan\n",
    "wdf.loc[wdf.wind_bearing.isnull(), 'humidity']    = np.nan\n",
    "wdf.loc[wdf.wind_bearing.isnull(), 'wind_speed']  = np.nan\n",
    "wdf.loc[wdf.wind_bearing.isnull(), 'cloud_cover'] = np.nan\n",
    "\n",
    "wdf.loc[(wdf['wind_speed'] < 0) ,  'wind_speed'] = np.nan\n",
    "wdf.loc[wdf.wind_speed.isnull(), 'dew_point']   = np.nan\n",
    "wdf.loc[wdf.wind_speed.isnull(), 'apparent_temperature'] = np.nan\n",
    "wdf.loc[wdf.wind_speed.isnull(), 'humidity']    = np.nan\n",
    "wdf.loc[wdf.wind_speed.isnull(), 'wind_bearing']  = np.nan\n",
    "wdf.loc[wdf.wind_speed.isnull(), 'cloud_cover'] = np.nan\n",
    "\n",
    "wdf.set_index('datetime', inplace=True)\n",
    "wdf.interpolate(method='time', inplace=True)\n",
    "wdf = wdf.asfreq(\"15min\")\n",
    "wdf.interpolate(method='time', inplace=True)\n",
    "wdf.reset_index(inplace=True)\n",
    "wdf.drop_duplicates('datetime',inplace=True)\n",
    "wdf.reset_index(inplace=True,drop=True)\n",
    "wdf.drop(wdf[wdf.datetime.dt.date == pd.to_datetime(\"2016-02-29\").date()].index,inplace = True)\n",
    "wdf.shape # Shape will be 1,40,157   10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending load and weather data\n",
    "wdf = wdf[wdf.datetime.dt.date <= train_date]\n",
    "wdf = wdf.append(wdf_forecast_data,sort=True)\n",
    "\n",
    "df  = pickle.load(open(\"tpddl_data\",\"rb\"))\n",
    "df = df[(df.datetime.dt.date <= train_date) & (df.datetime.dt.year >= 2015)]\n",
    "df.drop(df[df.datetime.dt.date == pd.to_datetime(\"2016-02-29\").date()].index,inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, wdf, on='datetime', how='outer')\n",
    "\n",
    "## Holidays removing\n",
    "holidays = pd.read_excel(\"holidays.xlsx\")\n",
    "df = df[~df[\"date\"].isin(pd.melt(holidays)[\"value\"].dt.date)]\n",
    "df.shape # 131136  13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plott(df):\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(df)\n",
    "    plt.show()\n",
    "    \n",
    "def removing_outliers(df):\n",
    "    df2 = df.copy()\n",
    "    df2.drop(df2.loc[df2.datetime.dt.year == 2018,\"apparent_temperature\"].loc[81445:81478,].index, inplace = True)\n",
    "    df2.drop(df2.loc[df2.datetime.dt.year == 2018,\"apparent_temperature\"].loc[81537:81571,].index, inplace = True)\n",
    "    df2.drop(df2.loc[df2.loc[:,\"dew_point\"] <= 0,:].index, inplace = True)\n",
    "    df2.reset_index(inplace=True, drop = True)\n",
    "    return df2\n",
    "\n",
    "def humidex(df):\n",
    "        Td = df['dew_point'] + 273.15\n",
    "        Tc = df['temperature']\n",
    "        a = 1/273.15\n",
    "        b = 1/Td\n",
    "        c = Tc + 0.555*((6.11*(np.exp(5417.75*(a-b))))-10)\n",
    "        return c\n",
    "    \n",
    "def calculate_RH( df):\n",
    "        Es = 6.11*10.0**((7.5*df['temperature']) / (237.7 + df['temperature']))\n",
    "        E = 6.11*10.0**((7.5*df['dew_point'])/(237.7 + df['dew_point']))\n",
    "        RH = (E/Es) * 100\n",
    "        return RH\n",
    "\n",
    "def sinwave(df, feat, divs):\n",
    "    return np.sin(df[str(feat)]*(2*np.pi/divs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_load_realted(data_frame):\n",
    "    \n",
    "    df = data_frame.copy()\n",
    "    # Adding time related features\n",
    "    df['hour']   = df.datetime.dt.hour + 1   # Categorical\n",
    "    df['date']   = df.datetime.dt.date       # Dates dont consider this    ( No repeatation )\n",
    "    df['dom' ]   = df.datetime.dt.day        # Categorical 30\n",
    "    df['month']  = df.datetime.dt.month      # Categorical 12\n",
    "    df['year']   = df.datetime.dt.year       # Categorical dont consider this 12\n",
    "    df['dow' ]   = df.datetime.dt.dayofweek  # Categorical 6\n",
    "    df['doy' ]   = df.datetime.dt.dayofyear  # Categorical may be consider ( No repeatation )\n",
    "    df['tb']     = df.datetime.apply(lambda x : ((x.hour*60 + x.minute)//15+1)) # Time blocks 96\n",
    "    df['weekend']= np.where(df['dow']>5, 1,0)  # Imbalanced category\n",
    "\n",
    "    # This feature is add\n",
    "    df['woy']    = df.datetime.dt.weekofyear # Categorical 52\n",
    "    \n",
    "    df['lag2h'] = df.load.shift(8)\n",
    "    df['lag4h'] = df.load.shift(16)\n",
    "    \n",
    "    df['load_ewm1'] = df['load'].ewm(span = 6).mean()\n",
    "    df['load_ewm2'] = df['load'].ewm(span =10).mean()\n",
    "    \n",
    "    df['sin_doy']   = sinwave(df, \"doy\" , 365)\n",
    "    df['sin_tb']    = sinwave(df, \"tb\"  ,  96)\n",
    "    df['sin_dow']   = sinwave(df, \"dow\" ,   7)\n",
    "    df['sin_month'] = sinwave(df, \"month\", 12)\n",
    "    df[\"sin_woy\"]   = sinwave(df, \"woy\"  , 52) # Adding\n",
    "    \n",
    "    hour_mean = df.groupby(['date','hour']).mean()[\"load\"].reset_index().rename(columns = {'load':'hour_mean'})\n",
    "    df = pd.merge(df, hour_mean, on=['date','hour'], how='left')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_weather_realted(data_frame):\n",
    "    span = 32\n",
    "\n",
    "    df = data_frame.copy()\n",
    "    df['humidity'] = ((6.11*10.0**(7.5*df.dew_point/(237.7+df.dew_point)))*100)/((df.temperature + 273.15)*461.5)\n",
    "    df['humidex'] = humidex(df)\n",
    "    df['RH'] = calculate_RH(df)\n",
    "\n",
    "    df['tb_aptemp']   = df.tb   * df.apparent_temperature\n",
    "    df['tb_load']     = df.tb   * df.load\n",
    "    df['load_aptemp'] = df.load * df.apparent_temperature\n",
    "    \n",
    "    df['aptemp_ewm_6h']  = df['apparent_temperature'].ewm(span=24).mean()\n",
    "    df['aptemp_ewm']     = df['apparent_temperature'].ewm(span=span).mean()\n",
    "    df['temperature_ewm'] = df['temperature'].ewm(span=span).mean()\n",
    "    df['dew_point_ewm']   = df['dew_point'].ewm(span=span).mean()\n",
    "    df['humidity_ewm']    = df['humidity'].ewm(span=span).mean()\n",
    "    df['wind_speed_ewm']  = df['wind_speed'].ewm(span=span).mean()\n",
    "    df['cloud_cover_ewm'] = df['cloud_cover'].ewm(span=span).mean()\n",
    "    df['wind_bearing_ewm']       = df['wind_bearing'].ewm(span=span).mean()\n",
    "    df['precip_intensity_ewm']   = df['precip_intensity'].ewm(span=span).mean()\n",
    "    df['precip_probability_ewm'] = df['precip_probability'].ewm(span=span).mean()\n",
    "    df['load_ewm']             = df['load'].ewm(span=span).mean()\n",
    "    \n",
    "    \n",
    "    df['load_aptemp_ewm6']  = df['load_aptemp'].ewm(span=24).mean()\n",
    "    \n",
    "    df['wt']=df.dow*df.hour\n",
    "    df['tm']=df.temperature*df.month\n",
    "    df['tm2']=(df.temperature**2)\n",
    "    df['th']=df.temperature*df.hour\n",
    "    df['th2']=(df.temperature**2)*df.hour\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = removing_outliers(df)\n",
    "df = feature_engineering_load_realted(df)\n",
    "df = feature_engineering_weather_realted(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_feature = ['apparent_temperature','temperature','humidity',#'dew_point',\n",
    "'wind_speed','cloud_cover','wind_bearing','precip_probability','precip_intensity',\n",
    "'humidex','RH','tb_aptemp',#'load_aptemp','tb_load',\n",
    "'aptemp_ewm_6h','aptemp_ewm','temperature_ewm','dew_point_ewm','humidity_ewm','wind_speed_ewm','cloud_cover_ewm',\n",
    "'wind_bearing_ewm','precip_intensity_ewm','precip_probability_ewm',\n",
    "#'sdtbrm','sdtbrm2','3tbrm','3tbrmw',\n",
    "'load_aptemp_ewm6','load_aptemp_ewm12','load_aptemp_ewm24']\n",
    "\n",
    "'dew_point' === 'humidity'              0.99   # Take humidity\n",
    "'dew_point_ewm' === 'humidity_ewm'      0.99   #\n",
    "'humidity' === 'humidity_ewm'           0.99\n",
    "'dew_point' === 'dew_point_ewm'         0.99\n",
    "'aptemp_ewm_6h' === 'aptemp_ewm'        0.99\n",
    "'aptep_ewm_6h' === 'aptemp'             0.97\n",
    "'load_aptemp_12h' === 'load_aptemp_24h' 0.99\n",
    "'load_aptemp_6h' === 'load_aptemp_12h'  0.99\n",
    "\n",
    "'load_aptemp_12' === \"load_aptemp6\" == 0.99\n",
    "'load_aptemp_6' === 'load_aptemp' ==0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,18))\n",
    "sns.heatmap(df[weather_feature].corr(),square=True,annot=True,cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['datetime','load', 'lag2h','lag4h', 'sin_woy','sin_tb','sin_doy', 'dow','doy','month','hour',\n",
    "'apparent_temperature','humidity','wind_speed','cloud_cover','temperature','RH','humidex',\n",
    "'hour_mean',  'aptemp_ewm','load_ewm','wind_speed_ewm','cloud_cover_ewm',\n",
    "'load_aptemp','wt','tm','tm2',\n",
    "#'wind_bearing_ewm','precip_intensity_ewm','precip_probability_ewm' # Adding\n",
    "           ]\n",
    "\n",
    "shift_features = ['apparent_temperature','humidity','wind_speed','cloud_cover','temperature','RH','humidex',\n",
    "                  'aptemp_ewm','wind_speed_ewm','cloud_cover_ewm',\n",
    "                  'wind_bearing_ewm','precip_intensity_ewm','precip_probability_ewm', # Adding\n",
    "                 'tb_aptemp','doy','month','dow', 'sin_doy','sin_month','sin_woy','wt','tm','tm2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features  ['datetime','load',    'lag1','lag2','lag3','lag5',    'sin_tb','sin_doy','sin_woy'\n",
    "           'dow','doy','month','year','hour',\n",
    "           'apparent_temperature','humidity','wind_speed','cloud_cover','temperature',\"dew_point\",\"RH\",'humidex'\n",
    "           'hour_mean',\n",
    "           \n",
    "           'aptemp_mean_12h','aptemp_mean_6h'\n",
    "        ,'tb_aptemp','tb_load'\n",
    "        'load_aptemp', 'load_ewm' #'load_aptemp_ewm12','load_aptemp_ewm6',\n",
    "\n",
    "           #'temperature_ewm'\n",
    "           'tm4','hm12','dp_ewm','3tbrmw','lagcwm','wsp_12','cc_12',\\\n",
    "            'wt','tm','tm2','tm3','']\n",
    "\n",
    "\n",
    "shift_features  ['temperature','apparent_temperature','humidex','wind_speed','cloud_cover','dew_point','RH','humidity'\n",
    "                 'temperature_ewm','dp_ewm','aptemp_mean_6h','aptemp_mean_12h',\n",
    "                 'dow','doy','month',\n",
    "                 'tb_aptemp',,'hm12','\n",
    "                ,'wsp_12','cc_12','wci','wt','tm','tm2','tm3','tm4']\n",
    "categorical_features  ['dow','month','year','hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in shift_features:\n",
    "    df[i] = df[i].shift(-192)\n",
    "\n",
    "df = df[features].copy()\n",
    "df.dropna(inplace=True)\n",
    "df['target'] = df.load.shift(-192)\n",
    "\n",
    "\n",
    "validation = 15 * 96\n",
    "\n",
    "x_train, y_train   = df[df.target.notna()].iloc[:-validation,1:-1], df[df.target.notna()].iloc[:-validation,-1]\n",
    "x_valid, y_valid   = df[df.target.notna()].iloc[-validation:,1:-1], df[df.target.notna()].iloc[-validation:,-1] \n",
    "x_test = df.iloc[-96:].iloc[:,1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joblib.dump(model,path)\n",
    "model = joblib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set default parameters like this\n",
    "params = { \"learning_rate\" : 0.02,\n",
    "         \"n_estimators\" : 1200,\n",
    "         \"max_depth\" : 9,        #Tuned\n",
    "         \"min_child_weight\" : 3, #Tuned\n",
    "         \"gamma\" : 0.5,          #Tuned\n",
    "         \"subsample\" : 0.6,      #Tuned\n",
    "         \"colsample_bytree\" : 0.5, # Tuned\n",
    "         \"objective\" : \"reg:squarederror\",\n",
    "         \"verbosity\" : 1\n",
    "         }\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**params)\n",
    "\n",
    "dm_train = xgb.DMatrix(x_train,y_train)\n",
    "dm_valid = xgb.DMatrix(x_valid,y_valid)\n",
    "dm_test  = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.train(params, dm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These two used to see eval_set error just pass both of this into fit method\n",
    "eval_set  = [ (x_train, y_train) , (x_valid, y_valid)]\n",
    "eval_metric = [\"rmse\",\"mae\"]\n",
    "\n",
    "# Training start of XGBoost\n",
    "xgb_model.fit(x_train, y_train,\n",
    "        #eval_set = eval_set,\n",
    "        #eval_metric = eval_metric,\n",
    "        verbose=True)\n",
    "\n",
    "#evals_result = xgb_model.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return (np.sum(np.abs(y_true - y_pred)/y_true)*100) / len(y_pred)\n",
    "\n",
    "def mape_error(y_pred, dm_train):\n",
    "    y_true = dm_train.get_label()\n",
    "    return \"mape_error\" , float((np.sum(np.abs(y_true - y_pred)/y_true)*100) / len(y_pred))\n",
    "\n",
    "# pass xgb.train(feval=mape_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It shows the validation vs training MAE error with respect to n_estimators\n",
    "plt.figure(figsize = (20,6))\n",
    "pd.DataFrame(evals_result['validation_0']).mae.plot(label=\"train\")\n",
    "pd.DataFrame(evals_result['validation_1']).mae.plot(label=\"valid\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {learning_rate = 0.1,\n",
    "              n_estimators = 1000,\n",
    "              max_depth = 9,           #Tuned\n",
    "              min_child_weight = 3,    #Tuned\n",
    "              gamma = 0.5,             #Tuned\n",
    "              subsample = 0.6,         #Tuned\n",
    "              colsample_bytree = 0.5,  #Tuned\n",
    "              objective = \"reg:squarederror\",\n",
    "              verbosity = 1\n",
    "}\n",
    "\n",
    "eval_set  = [ (x_train, y_train) , (x_valid, y_valid)]\n",
    "eval_metric = mape_error\n",
    "\n",
    "clf = xgb.XGBModel(**param_dist)\n",
    "clf.fit(x_train, y_train,\n",
    "        eval_set = eval_set,\n",
    "        eval_metric = eval_metric,\n",
    "        verbose=False)\n",
    "\n",
    "evals_result = clf.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.train({'tree_method': 'hist', 'seed': 1994,\n",
    "           'disable_default_eval_metric': 1},\n",
    "          dtrain=dtrain,\n",
    "          num_boost_round=10,\n",
    "          obj=squared_log,\n",
    "          feval=rmsle,\n",
    "          evals=[(dtrain, 'dtrain'), (dtest, 'dtest')],\n",
    "          evals_result=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = raw_df[raw_df.datetime.dt.date == prediction_date.date()].load.values\n",
    "y_pred = xgb_model.predict(x_test)\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(y_true,label=\"actual\")\n",
    "plt.plot(y_pred,label=\"predict\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_store = pickle.load(open(\"mape_store\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_store.append((prediction_date.date(),mean_absolute_percentage_error(y_true,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"datetime\": np.array(mape_store)[:,0], \"mape_new\":np.array(mape_store)[:,1]})\n",
    "results.sort_values(\"datetime\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_store.append(((prediction_date + timedelta(2)).date(), 5.3449075403589275))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mape_store,open(\"mape_store\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you get 0.8 for both of the params then try this\n",
    "'subsample':[0.75, 0.8, 0.85],\n",
    "'colsample_bytree':[0.75, 0.8, 0.85]\n",
    "    \n",
    "# Try to tune regularizations \n",
    "'reg_alpha':[0.0005, 0.02, 0.1, 1, 100]\n",
    "    \n",
    "# We got optimum 0.1\n",
    "'reg_alpha':[0.001, 0.005, 0.01, 0.05]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(model, x_train, y_train, useTrainCV = True, cv_folds = 5, early_stopping_rounds = 20 ):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        # This gives us a good start of n_estimators\n",
    "        print(\"Before Test n_estimator: \",model.get_xgb_params()['n_estimators'])\n",
    "        xgb_param = model.get_xgb_params()\n",
    "        dm_train = xgb.DMatrix(x_train,y_train)\n",
    "        cvresult = xgb.cv(xgb_param, dm_train,num_boost_round = xgb_model.get_params()['n_estimators'],\n",
    "                  nfold = cv_folds,\n",
    "                  metrics= [\"rmse\",\"mae\"],\n",
    "                  early_stopping_rounds = 5)\n",
    "        \n",
    "        print(\"After Test n_estimator: \",cvresult.shape[0])\n",
    "        model.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    # This gives us a score after training\n",
    "    print(\"Model fitting starts...\")\n",
    "    eval_set  = [ (x_train, y_train) , (x_valid, y_valid)]\n",
    "    eval_metric = [\"rmse\",\"mae\"] \n",
    "    model.fit(x_train, y_train, \n",
    "              eval_metric=eval_metric, \n",
    "              eval_set = eval_set)\n",
    "    \n",
    "    return model\n",
    "    #Predict training set:\n",
    "    y_pred = model.predict(y_train.values)\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print \"Accuracy : %.4g\" % metrics.accuracy_score(y_train.values, y_pred)\n",
    "    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob)\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree_method='gpu_hist', gpu_id=0\n",
    "\n",
    "\n",
    "params = {'n_estimators':500,\n",
    "          'objective':'reg:squarederror',\n",
    "          'booster':'gbtree',\n",
    "          'max_depth':50,\n",
    "          'learning_rate':0.2,\n",
    "          'colsample_bytree':0.6,\n",
    "          'colsample_bylevel':0.6,\n",
    "          'subsample':0.8,\n",
    "          'min_child_weight':1,\n",
    "          'gamma':0.5}\n",
    "            \n",
    "model = xgb.train(params, dm_train, num_boost_round=2000, evals=[(dm_test, \"Test\")], early_stopping_rounds=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference of Prediction date\n",
    "\n",
    "dm_predict = xgb.DMatrix(X_predict)\n",
    "y_pred = model.predict(dm_predict)\n",
    "\n",
    "# Getting actual load values\n",
    "y_actual = np.array(raw_df[raw_df.datetime.dt.date == prediction_date].load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "plot_importance(model, max_num_features=30, height=0.8, ax=ax)\n",
    "ax.grid(False)\n",
    "plt.title(\"XGBoost - Feature Importance\", fontsize=15)\n",
    "plt.savefig(\"feature_imp.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape = (sum((y_actual - y_pred)/y_actual)*100)/96\n",
    "mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(y_pred,label='Pred')\n",
    "plt.plot(y_actual,label='Actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBRegressor( objective = 'reg:squarederror')\n",
    "\n",
    "params = {\"n_estimators\" : [300,350,400,500,600,700,900],\n",
    " \"learning_rate\"    : [0.02, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30] ,\n",
    " \"max_depth\"        : [ 8, 10, 12, 15, 18, 30, 40],\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7],\n",
    " \"gamma\"            : [ 0.2, 0.4, 0.6, 0.8, 1, 1.5],\n",
    " \"colsample_bytree\" : [ 0.3, 0.4, 0.5, 0.6, 0.7,0.8,1 ],\n",
    " \"subsample\"        : [0.2, 0.4, 0.5, 0.6, 0.7],\n",
    "  \"reg_alpha\"       : [0, 0.5, 1],\n",
    "  \"reg_lambda\"      : [1, 1.5, 2, 3, 4.5],\n",
    "}\n",
    "\n",
    "\n",
    "#'colsample_bylevel' this is not present\n",
    "\n",
    "\n",
    "xgb_rscv = RandomizedSearchCV(xgb_clf, param_distributions = params,\n",
    "                             cv = 7, verbose = 3)\n",
    "\n",
    "model_xgb = xgb_rscv.fit(X_train,y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (model_xgb.best_score_, model_xgb.best_params_))\n",
    "\n",
    "print(model_xgb.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.savgol_filter(ea['final']['forecast'], 25, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test_1 = {\n",
    " 'gamma':[0.1,0.2, 0.3, 0.4, 0.5],\n",
    " 'max_depth':[9,10,11]\n",
    "}\n",
    "xgb_reg = RandomizedSearchCV(xgb_model, param_test_1, n_iter=10, verbose=2, cv=5, iid=True)\n",
    "\n",
    "print(\"Randomized search..\")\n",
    "search_time_start = time.time()\n",
    "xgb_reg.fit(x_train, y_train)\n",
    "print(\"Randomized search time:\", time.time() - search_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set default parameters like this\n",
    "xgb_model = xgb.XGBRegressor(learning_rate = 0.1,\n",
    "                            n_estimators = 100,\n",
    "                            max_depth = 9,\n",
    "                            min_child_weight = 3,\n",
    "                            gamma = 0.5,\n",
    "                            subsample = 0.8,\n",
    "                            colsample_bytree = 0.8,\n",
    "                            objective = \"reg:squarederror\",\n",
    "                            verbosity = 1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backcasting comparisions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DBinteractor import DBInteractor\n",
    "from functions import functions\n",
    "db_interactor = DBInteractor()\n",
    "fun = functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_from, date_to = fun.get_date(-44), fun.get_date(-30)\n",
    "date_from, date_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_live = db_interactor.get_forecast_live(30,date_from,date_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_vrx  = db_interactor.get_forecast(30,date_from,date_to,'vrx')\n",
    "forecast_vrl  = db_interactor.get_forecast(30,date_from,date_to,'vrl')\n",
    "forecast_live = db_interactor.get_forecast_live(30,date_from,date_to)\n",
    "forecast_all  = pd.DataFrame({\"datetime\": forecast_live.datetime,\n",
    "            \"vrx\":forecast_vrx.forecast,\"vrl\":forecast_vrl.forecast,\n",
    "             'live':forecast_live.forecast})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast_vvn = db_interactor.get_forecast(75,date_from,date_to,'vvn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_all[\"actual\"] =  raw_df[(raw_df.datetime.dt.date >= pd.to_datetime(date_from)) & \n",
    "       (raw_df.datetime.dt.date <= pd.to_datetime(date_to))].load.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forecast_vrx), len(forecast_vvn), len(forecast_vrl), len(forecast_live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['vrx'] = None\n",
    "results['vrl'] = None\n",
    "results['vlive'] = None\n",
    "\n",
    "for i in range(17,32):\n",
    "    date = pd.to_datetime(\"2019-12-\" + str(i)).date()\n",
    "    dataa = forecast_all[forecast_all.datetime.dt.date == date]\n",
    "    results['vrx'][results.datetime == date] = mean_absolute_percentage_error(dataa.actual, dataa.vrx) \n",
    "    results['vrl'][results.datetime == date] = mean_absolute_percentage_error(dataa.actual, dataa.vrl)\n",
    "    results['vlive'][results.datetime == date] = mean_absolute_percentage_error(dataa.actual, dataa.live)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.index = range(17,32)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(results.mape_new,label=\"NEW\")\n",
    "plt.plot(results.vrx,label=\"VRX\")\n",
    "plt.plot(results.vrl,label=\"VRL\")\n",
    "plt.plot(results.vlive,label=\"VLIVE\")\n",
    "plt.xticks(range(17,32))\n",
    "plt.xlabel(\"December 2019\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
