{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cab99fa21b5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m   \u001b[1;31m#Additional scklearn functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_search\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m   \u001b[1;31m#Perforing grid search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'cross_validation'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, useTrainCV = True, cv_folds=5, early_stopping_rounds = 50 ):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain   = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        \n",
    "        cvresult  = xgb.cv(xgb_param, xgtrain, num_boost_round = alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print \"\\nModel Report\"\n",
    "    print \"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions)\n",
    "    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob)\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Approach for Parameter Tuning\n",
    "We will use an approach similar to that of GBM here. The various steps to be performed are:\n",
    "\n",
    "1. Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum number of trees for this learning rate. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.\n",
    "2. Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.\n",
    "3. Tune regularization parameters (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.\n",
    "4. Lower the learning rate and decide the optimal parameters .\n",
    "\n",
    "#### Step 1: Fix learning rate and number of estimators for tuning tree-based parameters.\n",
    "\n",
    "In order to decide on boosting parameters, we need to set some initial values of other parameters. Lets take the following values:\n",
    "\n",
    "1. **max_depth = 5 :** This should be between 3-10. I’ve started with 5 but you can choose a different number as well. 4-6 can be good starting points.\n",
    "2. **min_child_weight = 1 :** A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.\n",
    "3. **gamma = 0 :** A smaller value like 0.1-0.2 can also be chosen for starting. This will anyways be tuned later.\n",
    "4. **subsample, colsample_bytree = 0.8 :** This is a commonly used used start value. Typical values range between 0.5-0.9.\n",
    "5. **scale_pos_weight = 1:** Because of high class imbalance.\n",
    "\n",
    "Please note that all the above are just initial estimates and will be tuned later. Lets take the default learning rate of 0.1 here and check the optimum number of trees using cv function of xgboost. The function defined above will do it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate = 0.1,\n",
    " n_estimators = 1000,\n",
    " max_depth = 5,\n",
    " min_child_weight = 1,\n",
    " gamma = 0,\n",
    " subsample = 0.8,\n",
    " colsample_bytree = 0.8,\n",
    " objective = 'binary:logistic',\n",
    " nthread = 4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb1, train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set default parameters like this\n",
    "xgb_model = xgb.XGBRegressor(learning_rate = 0.1,\n",
    "                            n_estimators = 100,\n",
    "                            max_depth = 5,\n",
    "                            min_child_weight = 1,\n",
    "                            gamma = 0,\n",
    "                            subsample = 0.8,\n",
    "                            colsample_bytree = 0.8,\n",
    "                            objective = \"reg:squarederror\",\n",
    "                            verbosity = 1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param = xgb_model.get_xgb_params()\n",
    "dm_train = xgb.DMatrix(x_train,y_train)\n",
    "dm_valid = xgb.DMatrix(x_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This give us a good start of n_estimators \n",
    "# It takes time\n",
    "cvresult = xgb.cv(xgb_param, dm_train,\n",
    "                  num_boost_round = xgb_model.get_params()['n_estimators'],\n",
    "                  nfold = 5,\n",
    "                  metrics= [\"rmse\",\"mae\"],\n",
    "                  early_stopping_rounds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=5, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we set this good n_estimators to our xgb_model\n",
    "xgb_model.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=5, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## These two used to see eval_set error just pass both of this into fit method\n",
    "eval_s = [ (x_train, y_train) , (x_valid,y_valid)]\n",
    "eval_metric=[\"rmse\",\"mae\"]\n",
    "\n",
    "xgb_model.fit(x_train,y_train,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9706367474877502"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the results with default params and good n_estimator value\n",
    "#train_pred = xgb_model.predict(x_train)\n",
    "xgb_model.score(x_train,y_train)  # This is R2 Score\n",
    "#zz = xgb_model.predict(x_valid)\n",
    "#xgb_model.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we tune this two params of booster using randomsearch\n",
    "# RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized search..\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] min_child_weight=5, max_depth=5 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. min_child_weight=5, max_depth=5, total= 1.1min\n",
      "[CV] min_child_weight=5, max_depth=5 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. min_child_weight=5, max_depth=5, total= 1.1min\n",
      "[CV] min_child_weight=5, max_depth=5 .................................\n",
      "[CV] .................. min_child_weight=5, max_depth=5, total= 1.0min\n",
      "[CV] min_child_weight=5, max_depth=5 .................................\n",
      "[CV] .................. min_child_weight=5, max_depth=5, total= 1.1min\n",
      "[CV] min_child_weight=5, max_depth=5 .................................\n",
      "[CV] .................. min_child_weight=5, max_depth=5, total= 1.0min\n",
      "[CV] min_child_weight=1, max_depth=7 .................................\n",
      "[CV] .................. min_child_weight=1, max_depth=7, total= 1.5min\n",
      "[CV] min_child_weight=1, max_depth=7 .................................\n",
      "[CV] .................. min_child_weight=1, max_depth=7, total= 1.6min\n",
      "[CV] min_child_weight=1, max_depth=7 .................................\n",
      "[CV] .................. min_child_weight=1, max_depth=7, total= 1.5min\n",
      "[CV] min_child_weight=1, max_depth=7 .................................\n",
      "[CV] .................. min_child_weight=1, max_depth=7, total= 1.5min\n",
      "[CV] min_child_weight=1, max_depth=7 .................................\n",
      "[CV] .................. min_child_weight=1, max_depth=7, total= 1.5min\n",
      "[CV] min_child_weight=1, max_depth=9 .................................\n",
      "[CV] .................. min_child_weight=1, max_depth=9, total= 2.0min\n",
      "[CV] min_child_weight=1, max_depth=9 .................................\n",
      "[CV] .................. min_child_weight=1, max_depth=9, total= 2.0min\n",
      "[CV] min_child_weight=1, max_depth=9 .................................\n",
      "[CV] .................. min_child_weight=1, max_depth=9, total= 2.0min\n",
      "[CV] min_child_weight=1, max_depth=9 .................................\n",
      "[CV] .................. min_child_weight=1, max_depth=9, total= 2.0min\n",
      "[CV] min_child_weight=1, max_depth=9 .................................\n",
      "[CV] .................. min_child_weight=1, max_depth=9, total= 2.0min\n",
      "[CV] min_child_weight=3, max_depth=7 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=7, total= 1.5min\n",
      "[CV] min_child_weight=3, max_depth=7 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=7, total= 1.6min\n",
      "[CV] min_child_weight=3, max_depth=7 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=7, total= 1.6min\n",
      "[CV] min_child_weight=3, max_depth=7 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=7, total= 1.6min\n",
      "[CV] min_child_weight=3, max_depth=7 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=7, total= 1.6min\n",
      "[CV] min_child_weight=3, max_depth=9 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=9, total= 2.1min\n",
      "[CV] min_child_weight=3, max_depth=9 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=9, total= 2.0min\n",
      "[CV] min_child_weight=3, max_depth=9 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=9, total= 2.0min\n",
      "[CV] min_child_weight=3, max_depth=9 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=9, total= 2.0min\n",
      "[CV] min_child_weight=3, max_depth=9 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=9, total= 2.0min\n",
      "[CV] min_child_weight=3, max_depth=5 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=5, total= 1.1min\n",
      "[CV] min_child_weight=3, max_depth=5 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=5, total= 1.2min\n",
      "[CV] min_child_weight=3, max_depth=5 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=5, total= 1.1min\n",
      "[CV] min_child_weight=3, max_depth=5 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=5, total= 1.0min\n",
      "[CV] min_child_weight=3, max_depth=5 .................................\n",
      "[CV] .................. min_child_weight=3, max_depth=5, total= 1.0min\n",
      "[CV] min_child_weight=1, max_depth=5 .................................\n"
     ]
    }
   ],
   "source": [
    "# We all ready have classifier and we just hypertune only this two params\n",
    "\n",
    "param = {'max_depth':[3,5,7,9],\n",
    "      'min_child_weight':[1,3,5],\n",
    "      }\n",
    "\n",
    "xgb_reg = RandomizedSearchCV(xgb_model, param, n_iter=10,verbose=2, cv=5,iid=True)\n",
    "\n",
    "print(\"Randomized search..\")\n",
    "search_time_start = time.time()\n",
    "xgb_reg.fit(x_train, y_train)\n",
    "print(\"Randomized search time:\", time.time() - search_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = xgb_reg.best_score_\n",
    "best_params = xgb_reg.best_params_\n",
    "print(\"Best score: {}\".format(best_score))\n",
    "print(\"Best params: \")\n",
    "for param_name in sorted(best_params.keys()):\n",
    "    print('%s: %r' % (param_name, best_params[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':[4,5,6],  # Best ke aas pass ke lena hai\n",
    " 'min_child_weight':[4,5,6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XGBRegressor' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-5f0774cb15ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#train_pred = xgb_model.predict(x_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#train_pred_prob =\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mxgb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'XGBRegressor' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "print(\"Model Report:\")\n",
    "print(\"accuracy: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator, param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch3.fit(train[predictors],train[target])\n",
    "\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[0.6, 0.7, 0.8, 0.9],\n",
    " 'colsample_bytree':[0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator, param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch4.fit(train[predictors],train[target])\n",
    "\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get 0.8 for both of the params then try this\n",
    "'subsample':[0.75, 0.8, 0.85],\n",
    "'colsample_bytree':[0.75, 0.8, 0.85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to tune regularizations \n",
    "'reg_alpha':[0.0005, 0.02, 0.1, 1, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got optimum 0.1\n",
    "'reg_alpha':[0.001, 0.005, 0.01, 0.05]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now decrease the learning rate and increase the n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, useTrainCV = True, cv_folds=5, early_stopping_rounds = 50 ):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain   = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        \n",
    "        cvresult  = xgb.cv(xgb_param, xgtrain, num_boost_round = alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print \"\\nModel Report\"\n",
    "    print \"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions)\n",
    "    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob)\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
