{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import fetch_rcv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test =  # load datasets\n",
    "\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "param_grid = {\n",
    "        'silent': [False],\n",
    "        'max_depth': [6, 10, 15, 20],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n",
    "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "        'gamma': [0, 0.25, 0.5, 1.0],\n",
    "        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
    "        'n_estimators': [100]}\n",
    "\n",
    "fit_params = {'eval_metric': 'mlogloss',\n",
    "              'early_stopping_rounds': 10,\n",
    "              'eval_set': [(x_valid, y_valid)]}\n",
    "\n",
    "\n",
    "\n",
    "rs_clf = RandomizedSearchCV(clf, param_grid, n_iter = 20,\n",
    "                            n_jobs=1, verbose=2, cv=2,\n",
    "                            fit_params=fit_params,\n",
    "                            scoring='neg_log_loss', refit=False, random_state=42)\n",
    "\n",
    "print(\"Randomized search..\")\n",
    "search_time_start = time.time()\n",
    "rs_clf.fit(x_train, y_train)\n",
    "print(\"Randomized search time:\", time.time() - search_time_start)\n",
    "\n",
    "best_score = rs_clf.best_score_\n",
    "best_params = rs_clf.best_params_\n",
    "print(\"Best score: {}\".format(best_score))\n",
    "print(\"Best params: \")\n",
    "for param_name in sorted(best_params.keys()):\n",
    "    print('%s: %r' % (param_name, best_params[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cross - validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "### load data in do training\n",
    "dtrain = xgb.DMatrix('../data/agaricus.txt.train')\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic'}\n",
    "num_round = 2\n",
    "\n",
    "print('running cross validation')\n",
    "# do cross validation, this will print result out as\n",
    "# [iteration]  metric_name:mean_value+std_value\n",
    "# std_value is standard deviation of the metric\n",
    "xgb.cv(param, dtrain, num_round, nfold=5,\n",
    "       metrics={'error'}, seed=0,\n",
    "       callbacks=[xgb.callback.print_evaluation(show_stdv=True)])\n",
    "\n",
    "print('running cross validation, disable standard deviation display')\n",
    "# do cross validation, this will print result out as\n",
    "# [iteration]  metric_name:mean_value\n",
    "res = xgb.cv(param, dtrain, num_boost_round=10, nfold=5,\n",
    "             metrics={'error'}, seed=0,\n",
    "             callbacks=[xgb.callback.print_evaluation(show_stdv=False),\n",
    "                        xgb.callback.early_stop(3)])\n",
    "print(res)\n",
    "print('running cross validation, with preprocessing function')\n",
    "# define the preprocessing function\n",
    "# used to return the preprocessed training, test data, and parameter\n",
    "# we can use this to do weight rescale, etc.\n",
    "# as a example, we try to set scale_pos_weight\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    return (dtrain, dtest, param)\n",
    "\n",
    "# do cross validation, for each fold\n",
    "# the dtrain, dtest, param will be passed into fpreproc\n",
    "# then the return value of fpreproc will be used to generate\n",
    "# results of that fold\n",
    "xgb.cv(param, dtrain, num_round, nfold=5,\n",
    "       metrics={'auc'}, seed=0, fpreproc=fpreproc)\n",
    "\n",
    "###\n",
    "# you can also do cross validation with customized loss function\n",
    "# See custom_objective.py\n",
    "##\n",
    "print('running cross validation, with cutomsized loss function')\n",
    "def logregobj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = 1.0 / (1.0 + np.exp(-preds))\n",
    "    grad = preds - labels\n",
    "    hess = preds * (1.0 - preds)\n",
    "    return grad, hess\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'error', float(sum(labels != (preds > 0.0))) / len(labels)\n",
    "\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1}\n",
    "# train with customized objective\n",
    "xgb.cv(param, dtrain, num_round, nfold=5, seed=0,\n",
    "       obj=logregobj, feval=evalerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBRegressor( objective = 'reg:squarederror')\n",
    "\n",
    "params = {\"n_estimators\" : [300,350,400,500,600,700,900],\n",
    " \"learning_rate\"    : [0.02, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30] ,\n",
    " \"max_depth\"        : [ 8, 10, 12, 15, 18, 30, 40],\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7],\n",
    " \"gamma\"            : [ 0.2, 0.4, 0.6, 0.8, 1, 1.5],\n",
    " \"colsample_bytree\" : [ 0.3, 0.4, 0.5, 0.6, 0.7,0.8,1 ],\n",
    " \"subsample\"        : [0.2, 0.4, 0.5, 0.6, 0.7],\n",
    "  \"reg_alpha\"       : [0, 0.5, 1],\n",
    "  \"reg_lambda\"      : [1, 1.5, 2, 3, 4.5],\n",
    "}\n",
    "\n",
    "\n",
    "#'colsample_bylevel' this is not present\n",
    "\n",
    "\n",
    "xgb_rscv = RandomizedSearchCV(xgb_clf, param_distributions = params,\n",
    "                             cv = 7, verbose = 3)\n",
    "\n",
    "model_xgb = xgb_rscv.fit(X_train,y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (model_xgb.best_score_, model_xgb.best_params_))\n",
    "\n",
    "print(model_xgb.score(x_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
